import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from xml.etree import ElementTree as ET
from concurrent.futures import ThreadPoolExecutor

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã¨Slack Webhook URLã‚’å–å¾—
PSI_API_KEY = os.environ.get("PSI_API_KEY")
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

# å¯¾è±¡ã‚µã‚¤ãƒˆã®èµ·ç‚¹URL
START_URL = "https://good-apps.jp/"

def normalize_url(url: str) -> str:
    """
    æŒ‡å®šã—ãŸURLã‚’æ­£è¦åŒ–ã—ã¦ã€å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®URLã®ã¿ã‚’è¿”ã™ã€‚
    - ã‚¹ã‚­ãƒ¼ãƒ ã‚„ãƒ›ã‚¹ãƒˆåã‚’è£œå®Œãƒ»çµ±ä¸€ã—ã€ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚„ã‚¯ã‚¨ãƒªã‚’é™¤å»ã€‚
    - good-apps.jpãƒ‰ãƒ¡ã‚¤ãƒ³å¤–ã®ãƒªãƒ³ã‚¯ã¯é™¤å¤–ã€‚
    """
    parsed = urlparse(url)
    scheme = parsed.scheme
    netloc = parsed.netloc
    path = parsed.path

    # ã‚¹ã‚­ãƒ¼ãƒ ãŒç„¡ã„å ´åˆã¯HTTPSã‚’ä»®å®š
    if scheme == "":
        scheme = "https"
    # HTTPã¯HTTPSã«çµ±ä¸€
    if scheme == "http":
        scheme = "https"
    # è¨±å¯ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆgood-apps.jp æœ¬ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿ï¼‰
    allowed_domains = ["good-apps.jp", "www.good-apps.jp"]
    if netloc == "":
        # ç›¸å¯¾URLã¯é™¤å¤–ï¼ˆäº‹å‰ã«urljoinã§çµ¶å¯¾URLåŒ–ã™ã‚‹æƒ³å®šï¼‰
        return None
    if netloc not in allowed_domains:
        return None

    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã¨ã‚¯ã‚¨ãƒªã‚’é™¤å»ã—ã¦URLå†æ§‹ç¯‰
    parsed = parsed._replace(scheme=scheme, netloc=netloc, fragment="", query="", params="")
    url_clean = parsed.geturl()
    # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€å‡¦ç†ï¼ˆãƒ«ãƒ¼ãƒˆä»¥å¤–ã®URLã¯æœ«å°¾ã®ã€Œ/ã€ã‚’å‰Šé™¤ï¼‰
    if url_clean.endswith("/") and parsed.path not in ["", "/"]:
        url_clean = url_clean[:-1]
    # ãƒ‘ã‚¹ãŒç©ºï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ç›´ä¸‹ï¼‰ã®å ´åˆã¯ "/" ã‚’ä»˜åŠ 
    if urlparse(url_clean).path == "":
        url_clean = url_clean + "/"
    return url_clean

def gather_all_urls() -> tuple[set, dict]:
    """
    ã‚µã‚¤ãƒˆå†…ã®å…¨ãƒšãƒ¼ã‚¸URLã‚’å–å¾—ã™ã‚‹ã€‚ã¾ãšã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è©¦ã¿ã€ç„¡ã„å ´åˆã¯ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã€‚
    æˆ»ã‚Šå€¤: (URLã‚»ãƒƒãƒˆ, ã‚¿ã‚¤ãƒˆãƒ«è¾æ›¸)
      - URLã‚»ãƒƒãƒˆ: ç™ºè¦‹ã—ãŸå…¨URLã®é›†åˆ
      - ã‚¿ã‚¤ãƒˆãƒ«è¾æ›¸: URLã‚’ã‚­ãƒ¼ã€ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ï¼ˆSlackå‡ºåŠ›ç”¨ï¼‰
    """
    urls = set()
    titles = {}

    # 1. ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å–å¾—ã¨è§£æ
    sitemap_urls = [
        "https://good-apps.jp/sitemap.xml",
        "https://good-apps.jp/sitemap_index.xml",
        "https://good-apps.jp/wp-sitemap.xml"
    ]
    sitemap_found = False
    for sitemap_url in sitemap_urls:
        try:
            resp = requests.get(sitemap_url, timeout=10)
        except Exception:
            continue
        if resp.status_code == 200 and resp.content:
            content = resp.content
            # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—XMLã‹ã©ã†ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
            if b"<urlset" in content or b"<sitemapindex" in content:
                sitemap_found = True
                try:
                    root = ET.fromstring(content)
                except ET.ParseError:
                    continue
                # XMLåå‰ç©ºé–“ã‚’è€ƒæ…®ã—ã¦ã‚¿ã‚°åã‚’å–å¾—
                root_tag = root.tag.split("}", 1)[-1]  # namespaceã‚’é™¤å»
                if root_tag == "urlset":
                    # å˜ä¸€ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼ˆç›´æ¥URLãƒªã‚¹ãƒˆï¼‰
                    for url_elem in root.findall(".//{*}loc"):
                        if url_elem.text:
                            norm = normalize_url(url_elem.text.strip())
                            if norm:
                                urls.add(norm)
                elif root_tag == "sitemapindex":
                    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå­ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è¾¿ã‚‹ï¼‰
                    for sitemap_elem in root.findall(".//{*}sitemap"):
                        loc = sitemap_elem.find("{*}loc")
                        if loc is not None and loc.text:
                            sub_sitemap_url = loc.text.strip()
                            try:
                                sub_resp = requests.get(sub_sitemap_url, timeout=10)
                            except Exception:
                                continue
                            if sub_resp.status_code == 200 and sub_resp.content:
                                try:
                                    sub_root = ET.fromstring(sub_resp.content)
                                except ET.ParseError:
                                    continue
                                for url_elem in sub_root.findall(".//{*}loc"):
                                    if url_elem.text:
                                        norm = normalize_url(url_elem.text.strip())
                                        if norm:
                                            urls.add(norm)
                # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰å–å¾—ã§ããŸURLã«ã¤ã„ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                for page_url in list(urls):
                    try:
                        page_resp = requests.get(page_url, timeout=5)
                        if page_resp.status_code == 200 and "text/html" in page_resp.headers.get("Content-Type", ""):
                            soup = BeautifulSoup(page_resp.text, "html.parser")
                            title_tag = soup.find("title")
                            if title_tag and title_tag.text:
                                titles[page_url] = title_tag.text.strip()
                    except Exception:
                        continue
                break  # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—æˆåŠŸã—ãŸã®ã§ãƒ«ãƒ¼ãƒ—é›¢è„±
    # 2. ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãŒç„¡ã‹ã£ãŸå ´åˆã€ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã§URLåé›†
    if not sitemap_found:
        from collections import deque
        start = normalize_url(START_URL)
        if start:
            urls.add(start)
            titles[start] = ""  # ã‚¿ã‚¤ãƒˆãƒ«ã¯å¾Œã§å–å¾—
            queue = deque([start])
        else:
            queue = deque()
        visited = set(urls)
        while queue:
            current_url = queue.popleft()
            try:
                resp = requests.get(current_url, timeout=10)
            except Exception:
                continue
            if resp.status_code != 200:
                continue
            content_type = resp.headers.get("Content-Type", "")
            if "text/html" not in content_type:
                continue
            # HTMLã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒªãƒ³ã‚¯æŠ½å‡º
            try:
                soup = BeautifulSoup(resp.text, "html.parser")
            except Exception:
                continue
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup.find("title")
            if title_tag and title_tag.text:
                titles[current_url] = title_tag.text.strip()
            # å…¨ã¦ã®<a>ã‚¿ã‚°ã‚’èµ°æŸ»ã—ã€æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            for a in soup.find_all("a", href=True):
                href = a["href"]
                new_url = urljoin(current_url, href)
                norm = normalize_url(new_url)
                if not norm:
                    continue
                if norm in visited:
                    continue
                # ç”»åƒã‚„ã‚¢ã‚»ãƒƒãƒˆç­‰ã®ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                if norm.lower().endswith((
                    ".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp",
                    ".pdf", ".zip", ".rar", ".7z", ".tar", ".gz",
                    ".css", ".js", ".json", ".xml", ".mp4", ".mp3"
                )):
                    continue
                visited.add(norm)
                urls.add(norm)
                queue.append(norm)
    return urls, titles

# åé›†ã—ãŸURLä¸€è¦§ã‚’å–å¾—
all_urls, titles = gather_all_urls()

# 1ãƒšãƒ¼ã‚¸ã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆçµ‚äº†
if not all_urls:
    print("å¯¾è±¡URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    exit(0)

# Core Web Vitalsã§ã€Œé…ã„ã€ã¨åˆ¤å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
issues = []  # (url, {strategy: {metric: value, ...}, ...}) ã®ãƒªã‚¹ãƒˆ

def check_url(url: str) -> tuple[str, dict]:
    """æŒ‡å®šURLã®ãƒ¢ãƒã‚¤ãƒ«ãƒ»ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®Core Web Vitalsã‚’ãƒã‚§ãƒƒã‚¯ã—ã€é…ã„æŒ‡æ¨™ãŒã‚ã‚Œã°çµæœã‚’è¿”ã™"""
    result = {}
    for strategy in ["mobile", "desktop"]:
        # PageSpeed Insights API å‘¼ã³å‡ºã—URLã‚’æ§‹ç¯‰
        api_endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&strategy={strategy}"
        if PSI_API_KEY:
            api_endpoint += f"&key={PSI_API_KEY}"
        try:
            res = requests.get(api_endpoint, timeout=30)
        except Exception as e:
            print(f"[Error] PSI API request failed: {url} ({strategy}) - {e}")
            continue
        if res.status_code != 200:
            print(f"[Warning] PSI API returned {res.status_code} for {url} ({strategy})")
            continue
        data = res.json()
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®æŒ‡æ¨™ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ç¢ºèª
        metrics = data.get("loadingExperience", {}).get("metrics", {})
        if not metrics:
            continue  # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆæ–°ã—ã„ãƒšãƒ¼ã‚¸ç­‰ï¼‰
        slow_metrics = {}
        # LCP, FID, CLSã«ã¤ã„ã¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’åˆ¤å®š
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼å€¤ãŒ "SLOW" ã®å ´åˆã®ã¿è¨˜éŒ²
        # percentileå€¤ã‹ã‚‰äººé–“èª­ã¿å¯èƒ½ãªå€¤ï¼ˆç§’ãƒ»ãƒŸãƒªç§’ï¼‰ã«å¤‰æ›
        if "LARGEST_CONTENTFUL_PAINT_MS" in metrics:
            cat = metrics["LARGEST_CONTENTFUL_PAINT_MS"].get("category")
            if cat == "SLOW":
                val_ms = metrics["LARGEST_CONTENTFUL_PAINT_MS"].get("percentile")
                if isinstance(val_ms, (int, float)):
                    slow_metrics["LCP"] = f"{val_ms/1000:.1f}ç§’"
                else:
                    slow_metrics["LCP"] = f"{val_ms}ç§’"
        if "FIRST_INPUT_DELAY_MS" in metrics:
            cat = metrics["FIRST_INPUT_DELAY_MS"].get("category")
            if cat == "SLOW":
                val_ms = metrics["FIRST_INPUT_DELAY_MS"].get("percentile")
                if isinstance(val_ms, (int, float)):
                    slow_metrics["FID"] = f"{int(val_ms)}ãƒŸãƒªç§’"
                else:
                    slow_metrics["FID"] = f"{val_ms}ãƒŸãƒªç§’"
        if "CUMULATIVE_LAYOUT_SHIFT_SCORE" in metrics:
            cat = metrics["CUMULATIVE_LAYOUT_SHIFT_SCORE"].get("category")
            if cat == "SLOW":
                val_cls = metrics["CUMULATIVE_LAYOUT_SHIFT_SCORE"].get("percentile")
                if isinstance(val_cls, (int, float)):
                    # CLSã‚¹ã‚³ã‚¢ã¯ç™¾åˆ†ç‡è¡¨ç¤ºã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚æ•°å€¤ã‚’0ï½1ç¯„å›²ã«å¤‰æ›
                    cls_value = float(val_cls)
                    if cls_value > 1.0:
                        cls_value = cls_value / 100.0
                    slow_metrics["CLS"] = f"{cls_value:.2f}"
                else:
                    slow_metrics["CLS"] = str(val_cls)
        if slow_metrics:
            result[strategy] = slow_metrics
    return (url, result)

# ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§å…¨URLã®ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œï¼ˆAPIã‚³ãƒ¼ãƒ«ã‚’ä¸¦åˆ—åŒ–ã—ã¦æ™‚é–“çŸ­ç¸®ï¼‰
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(check_url, url) for url in all_urls]
    for future in futures:
        url, result = future.result()
        if result:
            issues.append((url, result))

# Slacké€šçŸ¥ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®çµ„ã¿ç«‹ã¦
message_lines = []
message_lines.append("*Core Web Vitalsãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çµæœ*")
if not issues:
    # å•é¡Œã®ã‚ã‚‹ãƒšãƒ¼ã‚¸ãŒç„¡ã„å ´åˆ
    message_lines.append("å…¨ãƒšãƒ¼ã‚¸ã®Core Web VitalsæŒ‡æ¨™ã¯è‰¯å¥½ã§ã—ãŸã€‚ğŸ‰")
else:
    message_lines.append("ä»¥ä¸‹ã®ãƒšãƒ¼ã‚¸ã§**ã€Œé…ã„ã€**ã¨åˆ¤å®šã•ã‚ŒãŸCore Web VitalsæŒ‡æ¨™ãŒã‚ã‚Šã¾ã™ï¼š")
    for url, result in issues:
        # Slackç”¨ã®ãƒªãƒ³ã‚¯æ›¸å¼ï¼ˆ<URL|è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ>ï¼‰ã€‚ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ç”¨
        title = titles.get(url, url)
        if title:
            # Slackã®ç‰¹æ®Šæ–‡å­— '|' ã‚’å…¨è§’ã«ç½®æ›ï¼ˆãƒªãƒ³ã‚¯è¡¨ç¤ºã®åŒºåˆ‡ã‚Šã¨è¡çªã—ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
            title = title.replace("|", "ï½œ")
        link_text = title if title else url
        link = f"<{url}|{link_text}>"
        # ãƒ¢ãƒã‚¤ãƒ«ãƒ»ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®å„çµæœã‚’ã¾ã¨ã‚ã‚‹
        parts = []
        if "mobile" in result and result["mobile"]:
            # è¤‡æ•°æŒ‡æ¨™ã¯èª­ç‚¹ã§åŒºåˆ‡ã‚Š
            metrics_list = [f"{m}é…ã„({val})" for m, val in result["mobile"].items()]
            parts.append("ãƒ¢ãƒã‚¤ãƒ« â€“ " + "ã€".join(metrics_list))
        else:
            parts.append("ãƒ¢ãƒã‚¤ãƒ« â€“ è‰¯å¥½")
        if "desktop" in result and result["desktop"]:
            metrics_list = [f"{m}é…ã„({val})" for m, val in result["desktop"].items()]
            parts.append("ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— â€“ " + "ã€".join(metrics_list))
        else:
            parts.append("ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— â€“ è‰¯å¥½")
        # ç®‡æ¡æ›¸ãã®å„è¡Œã‚’æ§‹ç¯‰
        message_lines.append(f"- {link}ï¼š{'; '.join(parts)}")

message_text = "\n".join(message_lines)

# Slackã«é€šçŸ¥ã‚’é€ä¿¡ï¼ˆWebhookã‚’ä½¿ç”¨ï¼‰
if SLACK_WEBHOOK_URL:
    try:
        resp = requests.post(SLACK_WEBHOOK_URL, json={"text": message_text})
        if resp.status_code != 200:
            print(f"[Error] Slacké€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ (status={resp.status_code}): {resp.text}")
    except Exception as e:
        print(f"[Error] Slacké€šçŸ¥ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")
else:
    # Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ¨™æº–å‡ºåŠ›ã«çµæœã‚’å‡ºåŠ›
    print(message_text)
